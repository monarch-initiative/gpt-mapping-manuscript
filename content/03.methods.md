## Methods

### Algorithm

Our method MapperGPT takes as input two ontologies *O1* and *O2* and a set of candidate mappings *M*.
These mappings are assumed to be have been generated from an existing high-recall method such as LOOM.

```
M' = {}
For m in M:
  prompt = GeneratePrompt(m.a, m.b, O1, O2)
  response = CompletePrompt(prompt, model)
  m' = Parse(response)
  add m' to M'
return M'
```

### Prompt generation

The method `GeneratePrompt` generates a prompt according to the following template:
    
{% raw %}
```
What is the relationship between the two specified concepts?

Give your answer in the form:

category: <one of: EXACT_MATCH, BROADER_THAN, NARROWER_THAN, RELATED_TO, DIFFERENT>
confidence: <one of: LOW, HIGH, MEDIUM>
similarities: <semicolon-separated list of similarities>
differences: <semicolon-separated list of differences>

Make use of all provided information, including the concept names, definitions, and relationships.

Examples:

{{ examples }}

Here are the two concepts:

{{ Describe(conceptA) }}
{{ Describe(conceptB) }}
```
{% endraw %}

The use of examples makes this a few-shot learning approach.

Examples are provided in-context in the following form:

```
[Concept A]
id: FOO:125
name: wing
def: part of a bird that is flapped to enable flight
is_a: Limb
relationship: part_of Bird
relationship: has_part Feather

[Concept B]
id: BAR:458
name: wing
relationship: part_of Aeroplane

category: DIFFERENT
confidence: HIGH
similarities: function
differences: A is an anatomical part; B is a part of a vehicle
```

For each candidate mapping between concepts A and B, we generate a description of each concept,
incorporating key elements: name, synonyms, definition, relationships. 

The `Describe` function will generate a textual description of an ontology or database concept, showing the
following properties:

- name
- synonyms
- definition
- parents (superclasses)
- other relationships

### Prompt Completion

The prompt is then passed to a GPT model, which generates a response.
In principle the method should work with any instruction-based model, either
local or remotely accessed via an API. In practice we have only
evaluated this against the OpenAI API and the two leading instruction-based
models, `gpt-3.5-turbo` and `gpt-4`.

### Response Parsing

The response is parsed to retrieve
the key data model elements: category, confidence, similarities, differences.

The result object can be exported to SSSOM format.

### Example

As an example, two concepts from the Drosophila (fruitfly) and zebrafish anatomy ontologies [@doi:10.1186/2041-1480-4-32, @doi:10.1186/2041-1480-5-12] are candidate matches
due to sharing a lexical element (the "PC" abbreviation). This is a false positive match in reality,
as the concept are entirely different.

The two concept descriptions are generated from respective ontologies as follows:

```yaml
[Concept A]
id: FBbt:00001906
name: embryonic/larval Malpighian tubule Type I cell
def: Type I cell of the embryonic/larval Malpighian tubules.
synonyms:  PC ;  embryonic/larval Malpighian tubule Type I cell ;  larval Malpighian tubule Type I cell ;  larval Malpighian tubule principal cell ; 
is_a:  embryonic/larval specialized Malpighian tubule cell ;  Malpighian tubule Type I cell ; 

[Concept B]
id: ZFA:0000320
name: caudal commissure
def: Diencephalic tract which is located in the vicinity of the dorsal diencephalon and mesencephalon and connects the pretectal nuclei. From Neuroanatomy of the Zebrafish Brain.
synonyms:  PC ;  caudal commissure ;  posterior commissure ; 
is_a:  diencephalic white matter ; 
relationship: part of synencephalon
relationship: start stage unknown
relationship: end stage adult
```

The response for this using gpt-3.5-turbo (August 2023) is:

```yaml
category: DIFFERENT
confidence: HIGH
similarities: NONE
differences: A is a type of cell in the embryonic/larval Malpighian tubules; B is a diencephalic tract in the zebrafish brain.
subject: FBbt:00001906
object: ZFA:0000320
```

This is then parsed to a YAML object:

```yaml
predicate: DIFFERENT
confidence: HIGH
similarities:
  - NONE
differences:
  - A is a type of cell in the embryonic/larval Malpighian tubules
  - B is a diencephalic tract in the zebrafish brain.
```

The consumer may typically only make use of the *predicate* slot, but the list of
similarities and differences may prove informative.

### Implementation

We use the Ontology Access Kit (OAK) library [@doi:10.5281/zenodo.8310471] to connect to a variety of ontologies in the Open Biological and Biomedical Ontology (OBO) Library [@doi:10.1093/database/baab069] and Bioportal [@pubmed:19483092]. 
OAK generally enables accessing ontologies, but we also make use of its ability to extract subsets of ontologies, 
perform lexical matching using OAK LexMatch, extract mappings from ontologies and ontology portals such as Bioportal, and
add labels to mapping tables which typically only include the mapped identifiers, for better readability. We make use of ROBOT [@doi:10.1186/s12859-019-3002-3] for converting between different ontology formats.
The overall mapping framework is implemented in OntoGPT [@doi:10.5281/zenodo.8278168] (https://github.com/monarch-initiative/ontogpt) in a method called `categorize-mappings`, where the input is a SSSOM mapping file (usually generated by a lexical matching tool) and 
the output is a SSSOM mapping file with `predicate_id` filled with predicted value. Example:

```bash
ontogpt categorize-mappings --model gpt-4 -i foo.sssom.tsv -o bar.sssom.tsv
```

The entire pipeline is implemented as a fully reproducible `Makefile` (https://github.com/cmungall/gpt-mapping-manuscript/blob/main/Makefile).

### Generation of test sets

To evaluate the method, we created a collection of test sets from the biological and biomedical domains.
We chose to devise new test sets as we wanted to base these on up-to-date, precise,
validated mappings derived from ontologies such as Mondo [@doi:10.1101/2022.04.13.22273750], Cell Ontology (CL) [@doi:10.1186/s13326-016-0088-7], and the Uberon Anatomy Ontology [@doi:10.1186/gb-2012-13-1-r5].

To generate anatomy test sets, we generated pairwise mappings between species-specific anatomy ontologies,
using the Uberon and CL mappings as the gold standard. 
If a pair of concepts are transitively linked
via Uberon or CL, then they are considered a match. 
For example, UBERON:0000924 (ectoderm) is mapped to FBbt:00000111 (ectoderm (fruitfly)) and ZFA:0000016 (ectoderm (zebrafish)), so we assume that FBbt:00000111 is an exact match to ZFA:0000016.
We used the same method for linking species-specifc developmental stage terms.

We also generated a renal disease test set by taking all heritable renal diseases from Mondo, all
renal diseases from NCIT, and generating a test set based on validated curated mappings between
Mondo and NCIT.

{{main_results_testsetsize}}
Table: Breakdown of the existing test sets.
{#tbl:main_results_testsetsize}

### Tool evaluation

We evaluate MapperGPT with two models: gpt-3.5-turbo and gpt-4. MapperGPT is capable of providing
refined predicates from SKOS but for this task we only take exactMatch as a predicted mapping,
and discard all others.

We also evaluated against the OAK lexmatch tool, as a high-recall baseline. Although lexmatch
allows for customizable rules, we ran this without any prior knowledge of the domains, and
considered any lexical match to be a predicted match.

We selected LogMap, which is one of the top-performing mappers in the OAEI. 
We convert LogMap results to SSSOM [doi@10.1093/database/baac035]. (Harshad to write)

LogMap produces a score with each mapping, so we scanned all scores to determine the optimal score threshold
in terms of accuracy (F1) (note this gives LogMap an advantage over our method, which does not produce a score).

